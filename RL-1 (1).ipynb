{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor:\n",
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "\n",
      "variable:\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[0., 0.],\n",
      "       [0., 0.]], dtype=float32)>\n",
      "\n",
      "Updated variable:\n",
      "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
      "array([[1., 1.],\n",
      "       [1., 1.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "\n",
    "# Define the Deep Q-Network (DQN) architecture\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define replay memory\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, transition):\n",
    "        self.memory.append(transition)\n",
    "        if len(self.memory) > self.capacity:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "# Define epsilon greedy exploration strategy\n",
    "def epsilon_greedy_action(policy_net, state, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax().item()\n",
    "\n",
    "# Define function to optimize the Q-network\n",
    "def optimize_model(policy_net, target_net, memory, batch_size, gamma, optimizer, loss_fn):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = zip(*transitions)\n",
    "    state_batch = torch.tensor(batch[0], dtype=torch.float32)\n",
    "    action_batch = torch.tensor(batch[1], dtype=torch.long)\n",
    "    reward_batch = torch.tensor(batch[2], dtype=torch.float32)\n",
    "    next_state_batch = torch.tensor(batch[3], dtype=torch.float32)\n",
    "    done_batch = torch.tensor(batch[4], dtype=torch.bool)\n",
    "\n",
    "    Q_values = policy_net(state_batch).gather(1, action_batch.unsqueeze(1)).squeeze(1)\n",
    "    next_Q_values = torch.zeros(batch_size)\n",
    "    next_Q_values[~done_batch] = target_net(next_state_batch[~done_batch]).max(1)[0].detach()\n",
    "    target_Q_values = reward_batch + (gamma * next_Q_values)\n",
    "\n",
    "    loss = loss_fn(Q_values, target_Q_values)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Define training function\n",
    "def train_dqn(env_name='CartPole-v1', num_episodes=1000, batch_size=64, gamma=0.99, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, target_update=10, memory_capacity=10000):\n",
    "    env = gym.make(env_name)\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    policy_net = DQN(input_dim, output_dim)\n",
    "    target_net = DQN(input_dim, output_dim)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    memory = ReplayMemory(memory_capacity)\n",
    "\n",
    "    epsilon = epsilon_start\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = epsilon_greedy_action(policy_net, torch.tensor(state, dtype=torch.float32), epsilon, output_dim)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            memory.push((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            optimize_model(policy_net, target_net, memory, batch_size, gamma, optimizer, loss_fn)\n",
    "\n",
    "        if episode % target_update == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
    "\n",
    "        print(f\"Episode {episode + 1}, Reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Train the DQN\n",
    "train_dqn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step Result: (array([-0.0405223 ,  0.20130984,  0.01130362, -0.24377534], dtype=float32), 1.0, False, False, {})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep Result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Unpack the returned values\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m result\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# If episode is finished, reset the environment\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
